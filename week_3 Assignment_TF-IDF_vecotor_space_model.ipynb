{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jF5bKU0PFE4iR-JWfOlkx0wztTm1Q638","authorship_tag":"ABX9TyMhsZvCmyBwLWZL9UeUAlZb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Required Libraries:"],"metadata":{"id":"wOU_9lN_PtGJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qw1806vIOEvO","executionInfo":{"status":"ok","timestamp":1726413423579,"user_tz":-345,"elapsed":7514,"user":{"displayName":"PRABESH PANDEY","userId":"10934286709721246056"}},"outputId":"74f130c2-d341-4eb4-90ab-c3458d1d9597"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import numpy as np\n","import math\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import os\n","import string\n","import logging\n","import re\n","from collections import defaultdict, Counter\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"markdown","source":["# 2. Loading Text Files:"],"metadata":{"id":"OvKF2IKYQrE5"}},{"cell_type":"code","source":["# Read documents from uploaded files\n","file_paths = [\"/content/drive/MyDrive/Information retrieval /week-3 Assignment, TF-IDF Vector Space Model\"]\n","\n","# Load documents into a list\n","docs = []\n","for file_path in file_paths:\n","    # Check if the path is a directory\n","    if os.path.isdir(file_path):\n","        # If it's a directory, iterate through its files\n","        for filename in os.listdir(file_path):\n","            filepath = os.path.join(file_path, filename)\n","            # Check if it's a file and then read\n","            if os.path.isfile(filepath):\n","                with open(filepath, 'r', encoding='utf-8') as file:\n","                    docs.append(file.read())\n","    # If it's not a directory, try reading it as a file\n","    elif os.path.isfile(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            docs.append(file.read())"],"metadata":{"id":"Gvd6nS2FQuUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Defining Queries:"],"metadata":{"id":"zKy_IGIFQ_dJ"}},{"cell_type":"code","source":[" # Define the queries\n"," queries = [\n","       \"Deep Learning \",\n","        \"Data mining \",\n","        \" machine learning\",\n","        \"Computer vision\",\n","        \"Artificial Inteliigence\",\n","        \" human \"\n","    ]"],"metadata":{"id":"PIUzNEXnRD3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Text Pre-Processing:"],"metadata":{"id":"F0dKdCAGcDhs"}},{"cell_type":"code","source":["# Function to lowercase and tokenize text\n","def tokenize(text):\n","    return text.lower().split()\n","\n"," # Convert to lowercase\n","    text = text.lower()\n","    # Remove special characters and punctuation\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","    # Remove digit\n","    text = re.sub(r\"\\d+\", \"\", text)"],"metadata":{"id":"2ozJfNjgcJgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize documents and queries\n","tokenized_docs = [tokenize(doc) for doc in docs]\n","tokenized_queries = [tokenize(query) for query in queries]\n","\n","# Create a vocabulary from the tokenized documents\n","vocab = list(set(term for doc in tokenized_docs for term in doc))\n","\n","\n","print(tokenized_docs)\n","print(tokenized_queries)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXDd40pncPW4","executionInfo":{"status":"ok","timestamp":1726416510636,"user_tz":-345,"elapsed":684,"user":{"displayName":"PRABESH PANDEY","userId":"10934286709721246056"}},"outputId":"960ef50e-9ef5-4684-de36-e3ed38cc81fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"mount_file_id\":\"1jf5bku0pfe4ir-jwfolkx0wzttm1q638\",\"authorship_tag\":\"abx9tymabycryeqdwnlheepnv36t\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"python', '3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"markdown\",\"source\":[\"#', '1.', 'required', 'libraries:\"],\"metadata\":{\"id\":\"wou_9ln_ptgj\"}},{\"cell_type\":\"code\",\"execution_count\":1,\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"qw1806vioevo\",\"executioninfo\":{\"status\":\"ok\",\"timestamp\":1726413423579,\"user_tz\":-345,\"elapsed\":7514,\"user\":{\"displayname\":\"prabesh', 'pandey\",\"userid\":\"10934286709721246056\"}},\"outputid\":\"74f130c2-d341-4eb4-90ab-c3458d1d9597\"},\"outputs\":[{\"output_type\":\"stream\",\"name\":\"stderr\",\"text\":[\"[nltk_data]', 'downloading', 'package', 'stopwords', 'to', '/root/nltk_data...\\\\n\",\"[nltk_data]', 'unzipping', 'corpora/stopwords.zip.\\\\n\",\"[nltk_data]', 'downloading', 'package', 'punkt', 'to', '/root/nltk_data...\\\\n\",\"[nltk_data]', 'unzipping', 'tokenizers/punkt.zip.\\\\n\",\"[nltk_data]', 'downloading', 'package', 'wordnet', 'to', '/root/nltk_data...\\\\n\"]}],\"source\":[\"import', 'numpy', 'as', 'np\\\\n\",\"import', 'math\\\\n\",\"import', 'nltk\\\\n\",\"nltk.download(\\'stopwords\\')\\\\n\",\"nltk.download(\\'punkt\\')\\\\n\",\"nltk.download(\\'wordnet\\')\\\\n\",\"import', 'os\\\\n\",\"import', 'string\\\\n\",\"import', 'logging\\\\n\",\"import', 're\\\\n\",\"from', 'collections', 'import', 'defaultdict,', 'counter\\\\n\",\"from', 'nltk.corpus', 'import', 'stopwords\\\\n\",\"from', 'nltk.tokenize', 'import', 'word_tokenize\\\\n\",\"from', 'nltk.stem', 'import', 'wordnetlemmatizer\"]},{\"cell_type\":\"markdown\",\"source\":[\"#', '2.', 'loading', 'text', 'files:\"],\"metadata\":{\"id\":\"ovkf2ikyqre5\"}},{\"cell_type\":\"code\",\"source\":[],\"metadata\":{\"id\":\"gvd6ns2fquup\"},\"execution_count\":null,\"outputs\":[]}]}'], ['title:', 'understanding', 'machine', 'learning', 'algorithms', 'content:', 'machine', 'learning', 'has', 'revolutionized', 'industries', 'by', 'enabling', 'systems', 'to', 'learn', 'from', 'data', 'and', 'make', 'decisions', 'without', 'explicit', 'programming.', 'supervised', 'learning,', 'unsupervised', 'learning,', 'and', 'reinforcement', 'learning', 'are', 'the', 'major', 'categories.', 'supervised', 'learning', 'is', 'the', 'most', 'common', 'approach,', 'where', 'models', 'are', 'trained', 'on', 'labeled', 'data...'], ['title:', 'the', 'history', 'of', 'artificial', 'intelligence', 'content:', 'artificial', 'intelligence', '(ai)', 'has', 'its', 'roots', 'in', 'the', '1950s', 'when', 'alan', 'turing', 'proposed', 'the', 'concept', 'of', 'machines', 'mimicking', 'human', 'intelligence.', 'the', 'field', 'has', 'since', 'seen', 'numerous', 'advancements,', 'such', 'as', 'expert', 'systems', 'in', 'the', '1980s,', 'and', 'more', 'recently,', 'the', 'rise', 'of', 'deep', 'learning...'], ['title:', 'introduction', 'to', 'natural', 'language', 'processing', 'content:', 'natural', 'language', 'processing', '(nlp)', 'bridges', 'the', 'gap', 'between', 'human', 'language', 'and', 'computer', 'understanding.', 'it', 'covers', 'tasks', 'such', 'as', 'language', 'translation,', 'sentiment', 'analysis,', 'and', 'named', 'entity', 'recognition.', 'recent', 'advancements', 'in', 'nlp', 'have', 'been', 'driven', 'by', 'deep', 'learning', 'models', 'like', 'transformers...'], ['title:', 'advancements', 'in', 'computer', 'vision', 'content:', 'computer', 'vision', 'enables', 'computers', 'to', 'interpret', 'and', 'make', 'decisions', 'based', 'on', 'visual', 'data.', 'with', 'the', 'development', 'of', 'convolutional', 'neural', 'networks', '(cnns),', 'the', 'accuracy', 'of', 'visual', 'recognition', 'tasks', 'has', 'greatly', 'improved.', 'applications', 'range', 'from', 'facial', 'recognition', 'to', 'self-driving', 'cars...'], ['title:', 'exploring', 'reinforcement', 'learning', 'content:', 'reinforcement', 'learning', '(rl)', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'where', 'agents', 'learn', 'to', 'make', 'decisions', 'by', 'interacting', 'with', 'an', 'environment.', 'key', 'concepts', 'in', 'rl', 'include', 'rewards,', 'states,', 'and', 'actions.', 'some', 'famous', 'applications', 'include', 'alphago,', 'where', 'rl', 'was', 'used', 'to', 'master', 'the', 'game', 'of', 'go...'], ['title:', 'data', 'preprocessing', 'for', 'machine', 'learning', 'content:', 'data', 'preprocessing', 'is', 'a', 'critical', 'step', 'in', 'any', 'machine', 'learning', 'pipeline.', 'it', 'involves', 'cleaning,', 'transforming,', 'and', 'organizing', 'data', 'so', 'that', 'models', 'can', 'process', 'it', 'efficiently.', 'techniques', 'include', 'handling', 'missing', 'values,', 'normalizing', 'data,', 'and', 'feature', 'scaling...'], ['title:', 'ethical', 'concerns', 'in', 'ai', 'content:', 'as', 'ai', 'systems', 'become', 'more', 'integrated', 'into', 'everyday', 'life,', 'ethical', 'concerns', 'about', 'bias,', 'privacy,', 'and', 'accountability', 'have', 'emerged.', \"it's\", 'crucial', 'for', 'developers', 'to', 'ensure', 'fairness', 'in', 'ai', 'models,', 'mitigate', 'biases', 'in', 'training', 'data,', 'and', 'adhere', 'to', 'ethical', 'guidelines...'], ['title:', 'the', 'role', 'of', 'big', 'data', 'in', 'ai', 'content:', 'big', 'data', 'refers', 'to', 'the', 'massive', 'volumes', 'of', 'data', 'generated', 'by', 'various', 'sources', 'such', 'as', 'social', 'media,', 'sensors,', 'and', 'transactions.', 'ai', 'thrives', 'on', 'large', 'datasets,', 'allowing', 'machine', 'learning', 'models', 'to', 'be', 'trained', 'more', 'accurately.', 'big', 'data', 'is', 'essential', 'for', 'training', 'modern', 'ai', 'systems...'], ['title:', 'introduction', 'to', 'data', 'mining', 'content:', 'data', 'mining', 'involves', 'extracting', 'patterns', 'from', 'large', 'datasets', 'to', 'generate', 'valuable', 'insights.', 'techniques', 'such', 'as', 'clustering,', 'classification,', 'and', 'association', 'rule', 'mining', 'are', 'often', 'used', 'to', 'uncover', 'hidden', 'relationships', 'in', 'data.', 'popular', 'data', 'mining', 'tools', 'include', 'weka', 'and', 'rapidminer...'], ['title:', 'trends', 'in', 'deep', 'learning', 'content:', 'deep', 'learning,', 'a', 'subset', 'of', 'machine', 'learning,', 'has', 'gained', 'significant', 'attention', 'in', 'recent', 'years', 'due', 'to', 'its', 'ability', 'to', 'solve', 'complex', 'problems.', 'key', 'developments', 'include', 'the', 'rise', 'of', 'generative', 'models,', 'transfer', 'learning,', 'and', 'the', 'use', 'of', 'unsupervised', 'learning', 'techniques', 'to', 'improve', 'model', 'performance...']]\n","[['deep', 'learning'], ['data', 'mining'], ['machine', 'learning'], ['computer', 'vision'], ['artificial', 'inteliigence'], ['human']]\n","['have', 'association', 'counter\\\\n\",\"from', 'transfer', 'emerged.', 'such', 'used', 'word_tokenize\\\\n\",\"from', 'master', 'any', 'everyday', 'due', 'trained', 'models', 'decisions', 'transactions.', 'self-driving', 'feature', 'accuracy', 'become', 'alphago,', 'life,', 'are', '1980s,', 'driven', 'files:\"],\"metadata\":{\"id\":\"ovkf2ikyqre5\"}},{\"cell_type\":\"code\",\"source\":[],\"metadata\":{\"id\":\"gvd6ns2fquup\"},\"execution_count\":null,\"outputs\":[]}]}', 'unzipping', 'missing', 'greatly', 'attention', 'text', 'by', 'facial', 'enabling', 'processing', 'package', 'interpret', 'networks', 'generated', 'revolutionized', 'tools', 'corpora/stopwords.zip.\\\\n\",\"[nltk_data]', 'algorithms', 'artificial', 'make', 'concepts', 'preprocessing', 'gained', 'improve', 'performance...', 'cars...', 'seen', 'some', 'nltk.tokenize', 'deep', 'include', 'without', 'an', 'biases', 'datasets,', 'ethical', 'understanding', 'natural', 'transforming,', 'its', 'computers', 'actions.', 'content:', 'advancements', 'training', 'named', 'tasks', 'it', 'nlp', 'categories.', 'classification,', 'major', 'model', 'handling', 'exploring', 'process', 'systems', 'advancements,', 'bias,', 'famous', 'is', 'on', 'alan', 'valuable', 'developments', '2.', 'years', 'states,', 'popular', 'neural', 'a', 'environment.', '1950s', 'proposed', 'sensors,', 'relationships', 'to', 'complex', 'concept', 'transformers...', 'common', 'human', 'applications', 'for', 'uncover', 'was', 'go...', 'numpy', 'solve', 'sources', 'media,', 'enables', 'generative', 'modern', 'efficiently.', 'guidelines...', 'analysis,', '{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"mount_file_id\":\"1jf5bku0pfe4ir-jwfolkx0wzttm1q638\",\"authorship_tag\":\"abx9tymabycryeqdwnlheepnv36t\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"python', 'thrives', 'clustering,', 'scaling...', 'rule', 'roots', 'numerous', 'development', 'values,', 'about', 'with', 'ability', 'supervised', 'models,', 'recently,', 'rise', 'fairness', 'insights.', 'collections', 'cleaning,', 'rapidminer...', 'can', 'wordnet', 'math\\\\n\",\"import', 'as', 'improved.', 'has', 'be', 'pandey\",\"userid\":\"10934286709721246056\"}},\"outputid\":\"74f130c2-d341-4eb4-90ab-c3458d1d9597\"},\"outputs\":[{\"output_type\":\"stream\",\"name\":\"stderr\",\"text\":[\"[nltk_data]', 'rewards,', 'convolutional', 'np\\\\n\",\"import', 'nltk\\\\n\",\"nltk.download(\\'stopwords\\')\\\\n\",\"nltk.download(\\'punkt\\')\\\\n\",\"nltk.download(\\'wordnet\\')\\\\n\",\"import', 'ai', 'various', 'involves', 'so', 'role', 'accurately.', 'interacting', 'significant', '3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"markdown\",\"source\":[\"#', 'learning', 'title:', 'understanding.', 'ensure', 'computer', '(cnns),', 'into', 'industries', 'hidden', 'key', 'history', 'gap', 'visual', '(nlp)', 'been', 'agents', 'problems.', 'the', 'generate', 'import', 'defaultdict,', 'stopwords', 'when', 'data...', 'machines', 'sentiment', 'step', 'data,', 'nltk.stem', 'crucial', 'field', 'systems...', 'intelligence', 'and', 'allowing', 'mimicking', 'bridges', 'developers', 'large', 'that', 'entity', 'subset', 'introduction', '(rl)', 'like', 'organizing', 'accountability', '/root/nltk_data...\\\\n\"]}],\"source\":[\"import', 'loading', 'in', 'normalizing', 'learning...', 'based', 'essential', 'unsupervised', 'language', 'tokenizers/punkt.zip.\\\\n\",\"[nltk_data]', '/root/nltk_data...\\\\n\",\"[nltk_data]', 'concerns', 'patterns', 'volumes', 'datasets', 'weka', 'recognition.', 'most', 'covers', 'extracting', 'approach,', 'social', 'expert', 'string\\\\n\",\"import', 'recognition', 'of', 'mitigate', 'nltk.corpus', 'techniques', 'wordnetlemmatizer\"]},{\"cell_type\":\"markdown\",\"source\":[\"#', 'turing', 'game', 'libraries:\"],\"metadata\":{\"id\":\"wou_9ln_ptgj\"}},{\"cell_type\":\"code\",\"execution_count\":1,\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"qw1806vioevo\",\"executioninfo\":{\"status\":\"ok\",\"timestamp\":1726413423579,\"user_tz\":-345,\"elapsed\":7514,\"user\":{\"displayname\":\"prabesh', 're\\\\n\",\"from', 'more', 'critical', 'required', 'downloading', 'os\\\\n\",\"import', 'logging\\\\n\",\"import', 'reinforcement', 'data.', 'recent', 'learning,', 'punkt', 'vision', 'machine', 'programming.', 'labeled', 'integrated', '1.', 'where', 'mining', 'often', \"it's\", 'refers', 'privacy,', 'adhere', 'trends', 'stopwords\\\\n\",\"from', 'explicit', '(ai)', 'from', 'data', 'intelligence.', 'between', 'translation,', 'range', 'rl', 'learn', 'pipeline.', 'use', 'big', 'massive', 'since']\n"]}]},{"cell_type":"markdown","source":["# 5. Term Frequency (TF)"],"metadata":{"id":"nZ-1Nvf2clwy"}},{"cell_type":"code","source":["# Function to calculate term frequency (TF)\n","def term_frequency(term, document):\n","    return document.count(term) / len(document)"],"metadata":{"id":"rPOQOyCicpRL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Inverse Document Frequency (IDF)"],"metadata":{"id":"rCKTSGCYcu1h"}},{"cell_type":"code","source":["# Function to calculate inverse document frequency (IDF)\n","def inverse_document_frequency(term, all_documents):\n","    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n","    return math.log(len(all_documents) / (1 + num_docs_containing_term))"],"metadata":{"id":"IiWa0JmTcx6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Computing TF-IDF"],"metadata":{"id":"S3Kt_GjSeBhV"}},{"cell_type":"code","source":["# Compute TF-IDF for a document\n","def compute_tfidf(document, all_documents, vocab):\n","    tfidf_vector = []\n","    for term in vocab:\n","        tf = term_frequency(term, document)\n","        idf = inverse_document_frequency(term, all_documents)\n","        tfidf_vector.append(tf * idf)\n","    return np.array(tfidf_vector)"],"metadata":{"id":"36f7q2N2eGZu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Cosine similarity between two vectors"],"metadata":{"id":"MazKlhoieJSZ"}},{"cell_type":"code","source":["# Compute cosine similarity between two vectors\n","def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm_vec1 = np.linalg.norm(vec1)\n","    norm_vec2 = np.linalg.norm(vec2)\n","    return dot_product / (norm_vec1 * norm_vec2)"],"metadata":{"id":"sx3X_a0ueMlm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Calculate TF-IDF vectors"],"metadata":{"id":"aiRhUiQDeP8z"}},{"cell_type":"code","source":["# Calculate TF-IDF vectors for documents and queries\n","doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n","query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]"],"metadata":{"id":"aGCGwln-eTM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Writing  Results to .textfile format:"],"metadata":{"id":"gcuoBsiEeWDd"}},{"cell_type":"code","source":["\n","file_path = \"/content/drive/MyDrive/Information retrieval /week-3 Assignment, TF-IDF Vector Space Model/result_Prabesh.txt\"\n","\n","with open(file_path, \"w\") as result_file:\n","    # Calculate cosine similarities and rank top 3 documents for each query\n","    for i, query_vector in enumerate(query_tfidf_vectors):\n","        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n","\n","        # Rank documents by similarity score\n","        ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)[:3]  # Top 3\n","\n","        # Prepare the result string\n","        result_str = f\"\\nTop 3 results for query '{queries[i]}':\\n\"\n","        for rank, (doc_index, score) in enumerate(ranked_docs, 1):\n","            result_str += f\"Rank {rank}: Document {doc_index + 1} with score {score:.4f}\\n\"\n","\n","        # Print to console and write to file\n","        print(result_str)\n","        result_file.write(result_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLyHN2Hre3bB","executionInfo":{"status":"ok","timestamp":1726417343487,"user_tz":-345,"elapsed":610,"user":{"displayName":"PRABESH PANDEY","userId":"10934286709721246056"}},"outputId":"9dd08efb-7261-48ec-cf8c-b91fa672b550"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top 3 results for query 'Deep Learning ':\n","Rank 1: Document 11 with score 0.2351\n","Rank 2: Document 4 with score 0.0950\n","Rank 3: Document 3 with score 0.0911\n","\n","\n","Top 3 results for query 'Data mining ':\n","Rank 1: Document 10 with score 0.6608\n","Rank 2: Document 9 with score 0.1209\n","Rank 3: Document 7 with score 0.1065\n","\n","\n","Top 3 results for query ' machine learning':\n","Rank 1: Document 2 with score 0.2157\n","Rank 2: Document 7 with score 0.1622\n","Rank 3: Document 6 with score 0.1328\n","\n","\n","Top 3 results for query 'Computer vision':\n","Rank 1: Document 5 with score 0.4220\n","Rank 2: Document 4 with score 0.0675\n","Rank 3: Document 1 with score 0.0000\n","\n","\n","Top 3 results for query 'Artificial Inteliigence':\n","Rank 1: Document 3 with score 0.3364\n","Rank 2: Document 1 with score 0.0000\n","Rank 3: Document 2 with score 0.0000\n","\n","\n","Top 3 results for query ' human ':\n","Rank 1: Document 3 with score 0.1282\n","Rank 2: Document 4 with score 0.1114\n","Rank 3: Document 1 with score 0.0000\n","\n"]}]}]}